### Machine learning models and techniques

**Bagging** (stands for Bootstrap Aggregation) is the way decrease the variance of your prediction by generating additional 
data for training from your original data set using combinations with repetitions to produce multi sets of the same 
carnality/size as your original data. By increasing the size of your training set you can't improve the model predictive 
force, but just decrease the variance, narrowly tuning the prediction to expected outcome.

**Boosting** is a an approach to calculate the output using several different models and then average the result using a weighted
average approach. By combining the advantages and pitfalls of these approaches by varying your weighting formula you can come 
up with a good predictive force for a wider range of input data, using different narrowly tuned models.
